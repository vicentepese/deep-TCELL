{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import torch \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import tokenizers\n",
    "\n",
    "from torch import tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import  RobertaConfig\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers import pre_tokenizers, normalizers, Tokenizer\n",
    "from tokenizers.normalizers import Lowercase, NFD\n",
    "from tokenizers.pre_tokenizers import ByteLevel, Whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (l1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(4050, 1032, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 1032, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 1032)\n",
       "      (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (key): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (value): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "              (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1032, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1032, bias=True)\n",
       "            (LayerNorm): LayerNorm((1032,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=1032, out_features=1032, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1032, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load settings\n",
    "with open('settings.json', 'r') as inFile:\n",
    "    settings = json.load(inFile)\n",
    "    \n",
    "# Set device \n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model = torch.load('best_model')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDR3Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, settings:dict, train:bool = True, label:str = None, tokenizer:tokenizers.Tokenizer=None, equal:bool=False) -> None:\n",
    "        cols = [\"activatedby_HA\", \"activatedby_NP\", \"activatedby_HCRT\", \"activated_any\", \"multilabel\", \"negative\"]\n",
    "        if label not in cols:\n",
    "            raise ValueError(\"Invalid label type. Expected one of %s\" % cols)\n",
    "        else: \n",
    "            self.label = label\n",
    "        if equal and label == \"num_label\":\n",
    "            raise ValueError(\"Equal size sets only allowed for binary classifications. num_label is multiclass.\")\n",
    "        \n",
    "        if train == True:\n",
    "            path_to_data = settings[\"file\"][\"train_data\"] \n",
    "        else:\n",
    "            path_to_data = settings[\"file\"][\"test_data\"]   \n",
    "              \n",
    "        self.path_to_data = path_to_data\n",
    "        self.data = pd.read_csv(self.path_to_data)\n",
    "        if equal == True:\n",
    "            min_sample=np.min(self.data[self.label].value_counts()) \n",
    "            data_pos = self.data[self.data[self.label]==1].sample(min_sample)\n",
    "            data_neg = self.data[self.data[self.label]==0].sample(min_sample)\n",
    "            self.data = pd.concat([data_pos, data_neg], ignore_index=True)\n",
    "        \n",
    "        if label == \"multilabel\":\n",
    "            self.labels = [0,1]\n",
    "            self.n_labels = 4\n",
    "        else:\n",
    "            self.labels = np.unique(self.data[[self.label]])\n",
    "            self.n_labels = len(self.labels)\n",
    "            \n",
    "        self.max_len = self.data.CDR3ab.str.len().max()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index:int):\n",
    "        if isinstance(self.tokenizer, tokenizers.Tokenizer):\n",
    "            self.tokenizer.enable_padding(length=self.max_len)\n",
    "            CDR3ab = \" \".join(list(self.data.CDR3ab[index]))\n",
    "            encodings = self.tokenizer.encode(CDR3ab)\n",
    "            item = {\n",
    "                \"ids\":tensor(encodings.ids, dtype=torch.long),\n",
    "                \"attention_mask\": tensor(encodings.attention_mask, dtype=torch.long), \n",
    "                \"CDR3ab\": self.data.CDR3ab[index]\n",
    "                }\n",
    "        else:\n",
    "            self.tokenizer.enable_padding(length=self.max_len)\n",
    "            encodings = self.tokenizer.encode(self.data.CDR3ab[index]) \n",
    "            item = {\n",
    "                \"ids\":tensor(encodings.ids, dtype=torch.long),\n",
    "                \"attention_mask\": tensor(encodings.attention_mask, dtype=torch.long),\n",
    "                \"CDR3ab\": self.data.CDR3ab[index]\n",
    "                }\n",
    "        if self.label == \"multilabel\":\n",
    "            item[\"target\"]=tensor(self.data[[\"activatedby_HA\", \"activatedby_NP\", \"activatedby_HCRT\", \"negative\"]].iloc[index],dtype =torch.long)\n",
    "        else:\n",
    "            item[\"target\"] = tensor(self.data[self.label][index], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tonekizer from tokenizers library \n",
    "if settings[\"param\"][\"tokenizer\"] == \"BPE\":\n",
    "    normalizer = normalizers.Sequence([Lowercase(), NFD()])\n",
    "    pre_tokenizer = pre_tokenizers.Sequence([ByteLevel()])\n",
    "    tokenizer = ByteLevelBPETokenizer(settings[\"tokenizer\"][\"BPE_vocab\"], settings[\"tokenizer\"][\"BPE_merge\"])\n",
    "    tokenizer.normalizer = normalizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizer\n",
    "elif settings[\"param\"][\"tokenizer\"] == \"WL\":\n",
    "    normalizer = normalizers.Sequence([Lowercase(), NFD()])\n",
    "    pre_tokenizer = pre_tokenizers.Sequence([Whitespace()])\n",
    "    tokenizer = Tokenizer(WordLevel()).from_file(settings[\"tokenizer\"][\"WL\"])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizer\n",
    "    tokenizer.normalizer = normalizer\n",
    "    tokenizer.enable_padding()\n",
    "else:\n",
    "    raise ValueError(\"Unknown tokenizer. Tokenizer argument must be BPE or WL.\")\n",
    "    \n",
    "# Create training and test dataset\n",
    "dataset_params={\"label\":settings[\"database\"][\"label\"], \"tokenizer\":tokenizer}\n",
    "train_data = CDR3Dataset(settings,train=True, equal=False, **dataset_params)\n",
    "test_data =CDR3Dataset(settings, train=False, **dataset_params)\n",
    "\n",
    "# Crate dataloaders\n",
    "loader_params = {'batch_size': 10,\n",
    "            'shuffle': True,\n",
    "            'num_workers': 0\n",
    "            }\n",
    "train_dataloader = DataLoader(train_data, **loader_params)\n",
    "test_dataloader = DataLoader(test_data, **loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDR3ab</th>\n",
       "      <th>activatedby_HA</th>\n",
       "      <th>activatedby_NP</th>\n",
       "      <th>activatedby_HCRT</th>\n",
       "      <th>negative</th>\n",
       "      <th>PROB_activatedby_HA</th>\n",
       "      <th>PROB_activatedby_NP</th>\n",
       "      <th>PROB_activatedby_HCRT</th>\n",
       "      <th>PROB_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAETFRGAQKLVF_CASSSTGNTGELFF</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAVGAGFGNEKLTF_CASSNVYKDVGGYTF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAVNAGGTSYGKLTF_CASSQGRMYEQYF</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAVNTGFQKLVF_CSAILAGGRQETQYF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAVETDSWGKLQF_CASSQDQGQTQPQHF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CALSDRGGSEKLVF_CASSLDGGSTDTQYF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CAVEADNYGQNFVF_CASSRPQGYDTQYF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CAMREGGTDKLIF_CASSLRTGVGAFF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CVVRTGGYQKVTF_CASSFQMERDTQYF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CALNTGGFKTIF_CASSYQGEEETQYF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           CDR3ab  activatedby_HA  activatedby_NP  \\\n",
       "0    CAETFRGAQKLVF_CASSSTGNTGELFF               1               0   \n",
       "1  CAVGAGFGNEKLTF_CASSNVYKDVGGYTF               0               0   \n",
       "2   CAVNAGGTSYGKLTF_CASSQGRMYEQYF               0               1   \n",
       "3    CAVNTGFQKLVF_CSAILAGGRQETQYF               0               0   \n",
       "4   CAVETDSWGKLQF_CASSQDQGQTQPQHF               0               0   \n",
       "5  CALSDRGGSEKLVF_CASSLDGGSTDTQYF               0               0   \n",
       "6   CAVEADNYGQNFVF_CASSRPQGYDTQYF               0               0   \n",
       "7     CAMREGGTDKLIF_CASSLRTGVGAFF               0               0   \n",
       "8    CVVRTGGYQKVTF_CASSFQMERDTQYF               0               0   \n",
       "9     CALNTGGFKTIF_CASSYQGEEETQYF               0               0   \n",
       "\n",
       "   activatedby_HCRT  negative  PROB_activatedby_HA  PROB_activatedby_NP  \\\n",
       "0                 0         0                0.999                  0.0   \n",
       "1                 0         1                0.000                  0.0   \n",
       "2                 0         0                0.000                  1.0   \n",
       "3                 0         1                0.000                  0.0   \n",
       "4                 0         1                0.000                  0.0   \n",
       "5                 0         1                0.000                  0.0   \n",
       "6                 1         0                0.000                  0.0   \n",
       "7                 0         1                0.000                  0.0   \n",
       "8                 0         1                0.000                  0.0   \n",
       "9                 1         0                0.000                  0.0   \n",
       "\n",
       "   PROB_activatedby_HCRT  PROB_negative  \n",
       "0                  0.000          0.002  \n",
       "1                  0.000          1.000  \n",
       "2                  0.001          0.000  \n",
       "3                  0.000          1.000  \n",
       "4                  0.000          1.000  \n",
       "5                  0.000          1.000  \n",
       "6                  0.960          0.024  \n",
       "7                  0.000          1.000  \n",
       "8                  0.000          1.000  \n",
       "9                  0.955          0.027  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 10 random CDRs and predict \n",
    "sample_train = next(iter(train_dataloader))\n",
    "\n",
    "# Predict \n",
    "torch.cuda.empty_cache() \n",
    "model.eval()\n",
    "outs_df = []\n",
    "ids = sample_train[\"ids\"].to(device)\n",
    "attention_mask = sample_train[\"attention_mask\"].to(device)\n",
    "targets = sample_train[\"target\"].to(device)\n",
    "outs = model(ids, attention_mask)\n",
    "\n",
    "# Bring to CPU \n",
    "targets = targets.to('cpu') .detach().numpy()\n",
    "outs = outs.to('cpu')\n",
    "outs = outs.detach().numpy()\n",
    "outs = np.around(outs, decimals=3) \n",
    "\n",
    "cols_prob = [\"PROB_activatedby_HA\", \"PROB_activatedby_NP\", \"PROB_activatedby_HCRT\", \"PROB_negative\"]\n",
    "cols = [\"activatedby_HA\", \"activatedby_NP\", \"activatedby_HCRT\", \"negative\"]\n",
    "\n",
    "# Crate dataframes \n",
    "outs_df = pd.DataFrame.from_records(outs)\n",
    "outs_df.columns = cols_prob\n",
    "\n",
    "# Crate dataframe of targets\n",
    "targets_df = pd.DataFrame.from_records(targets)\n",
    "targets_df.columns = cols\n",
    "targets_df.insert(0, 'CDR3ab', sample_train['CDR3ab'])\n",
    "\n",
    "# Concat \n",
    "comp_df = pd.concat([targets_df, outs_df], axis=1)\n",
    "comp_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDR3ab</th>\n",
       "      <th>activatedby_HA</th>\n",
       "      <th>activatedby_NP</th>\n",
       "      <th>activatedby_HCRT</th>\n",
       "      <th>negative</th>\n",
       "      <th>PROB_activatedby_HA</th>\n",
       "      <th>PROB_activatedby_NP</th>\n",
       "      <th>PROB_activatedby_HCRT</th>\n",
       "      <th>PROB_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAGRTDSWGKFQF_CSARDRWQQTSYEQYF</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAGQTNQGAQKLVF_CASRPLRVQETQYF</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAVMGNTGKLIF_CASSSGTSKDTQYF</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAVNTGGFKTIF_CSAELAGVSTDTQYF</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAVSDRTGGFKTIF_CASSLFDYEQYF</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CAVDPQAGTALIF_CASSEAGGSNQPQHF</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CAVETDSWGKLQF_CASSFTGSVGYTF</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CALMNNNAGNMLTF_CATSSGGGGKAYGYTF</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CAASKDSSYKLIF_CASSLGGGSETQYF</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CAAGDNDMRF_CASSQPGGGGANVLTF</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            CDR3ab  activatedby_HA  activatedby_NP  \\\n",
       "0   CAGRTDSWGKFQF_CSARDRWQQTSYEQYF               0               1   \n",
       "1    CAGQTNQGAQKLVF_CASRPLRVQETQYF               0               1   \n",
       "2      CAVMGNTGKLIF_CASSSGTSKDTQYF               1               1   \n",
       "3     CAVNTGGFKTIF_CSAELAGVSTDTQYF               0               1   \n",
       "4      CAVSDRTGGFKTIF_CASSLFDYEQYF               0               1   \n",
       "5    CAVDPQAGTALIF_CASSEAGGSNQPQHF               0               1   \n",
       "6      CAVETDSWGKLQF_CASSFTGSVGYTF               0               1   \n",
       "7  CALMNNNAGNMLTF_CATSSGGGGKAYGYTF               0               1   \n",
       "8     CAASKDSSYKLIF_CASSLGGGSETQYF               0               1   \n",
       "9      CAAGDNDMRF_CASSQPGGGGANVLTF               1               0   \n",
       "\n",
       "   activatedby_HCRT  negative  PROB_activatedby_HA  PROB_activatedby_NP  \\\n",
       "0                 0         0                0.000                0.000   \n",
       "1                 0         0                0.000                0.000   \n",
       "2                 0         0                0.000                0.001   \n",
       "3                 0         0                0.000                0.000   \n",
       "4                 0         0                0.000                0.000   \n",
       "5                 1         0                0.000                1.000   \n",
       "6                 0         0                0.000                0.000   \n",
       "7                 0         0                0.001                0.000   \n",
       "8                 0         0                0.001                0.000   \n",
       "9                 0         0                0.028                0.088   \n",
       "\n",
       "   PROB_activatedby_HCRT  PROB_negative  \n",
       "0                  0.000          1.000  \n",
       "1                  0.000          0.999  \n",
       "2                  0.882          0.078  \n",
       "3                  0.000          0.999  \n",
       "4                  0.000          1.000  \n",
       "5                  0.003          0.000  \n",
       "6                  0.000          0.999  \n",
       "7                  0.000          1.000  \n",
       "8                  0.000          0.999  \n",
       "9                  0.004          0.906  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get 10 random CDRs and predict \n",
    "sample_test = next(iter(test_dataloader))\n",
    "\n",
    "# Predict \n",
    "model.eval()\n",
    "outs_df = []\n",
    "ids = sample_test[\"ids\"].to(device)\n",
    "attention_mask = sample_test[\"attention_mask\"].to(device)\n",
    "targets = sample_test[\"target\"].to(device)\n",
    "outs = model(ids, attention_mask)\n",
    "\n",
    "# Bring to CPU \n",
    "targets = targets.to('cpu') .detach().numpy()\n",
    "outs = outs.to('cpu')\n",
    "outs = outs.detach().numpy()\n",
    "outs = np.around(outs, decimals=3) \n",
    "\n",
    "cols_prob = [\"PROB_activatedby_HA\", \"PROB_activatedby_NP\", \"PROB_activatedby_HCRT\", \"PROB_negative\"]\n",
    "cols = [\"activatedby_HA\", \"activatedby_NP\", \"activatedby_HCRT\", \"negative\"]\n",
    "\n",
    "# Crate dataframes \n",
    "outs_df = pd.DataFrame.from_records(outs)\n",
    "outs_df.columns = cols_prob\n",
    "\n",
    "# Crate dataframe of targets\n",
    "targets_df = pd.DataFrame.from_records(targets)\n",
    "targets_df.columns = cols\n",
    "targets_df.insert(0, 'CDR3ab', sample_test['CDR3ab'])\n",
    "\n",
    "# Concat \n",
    "comp_df = pd.concat([targets_df, outs_df], axis=1)\n",
    "comp_df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
